{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Mosquito Detection Tutorial\n\nThis tutorial shows how to use the `MosquitoDetector` from the CulicidaeLab\nlibrary to perform object detection on images. We will cover:\n\n- Loading the detector model\n- Preparing an image from the dataset\n- Running the model to get bounding boxes\n- Visualizing the results\n- Evaluating prediction accuracy\n- Running predictions on a batch of images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install the `culicidaelab` library if not already installed\n!pip install -q culicidaelab\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialization\n\nFirst, we'll get the global `settings` instance and use it to initialize our `MosquitoDetector`.\nBy setting `load_model=True`, we tell the detector to load the model weights into memory immediately.\nIf the model file doesn't exist locally, it will be downloaded automatically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom culicidaelab import get_settings\nfrom culicidaelab import MosquitoDetector, DatasetsManager\n\n# Get settings instance\nsettings = get_settings()\n\n# Initialize the datasets manager\nmanager = DatasetsManager(settings)\n\n# Load detection dataset\ndetect_data = manager.load_dataset(\"detection\", split=\"train[:20]\")\n\n# Instantiate the detector and load the model\nprint(\"Initializing MosquitoDetector and loading model...\")\ndetector = MosquitoDetector(settings=settings, load_model=True)\nprint(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Detecting Mosquitoes in a Dataset Image\n\nNow let's use an image from the detection dataset and run the detector on it.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect a detection sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "detect_sample = detect_data[5]\ndetect_image = detect_sample[\"image\"]\n\n# Get ground truth objects\nobjects = detect_sample[\"objects\"]\nprint(f\"Found {len(objects['bboxes'])} object(s) in this image.\")\n\n# The `predict` method returns a list of detections.\n# Each detection is a tuple: (x1, y1, x2, y2, confidence_score)\ndetections = detector.predict(detect_image)\n\n# The `visualize` method draws the bounding boxes onto the image for easy inspection.\nannotated_image = detector.visualize(detect_image, detections)\n\n# Display the result\nplt.figure(figsize=(12, 8))\nplt.imshow(annotated_image)\nplt.axis(\"off\")\nplt.title(\"Detected Mosquitoes\")\nplt.show()\n\n# Print the numerical detection results\nprint(\"\\nDetection Results:\")\nif detections:\n    for i, (x1, y1, x2, y2, conf) in enumerate(detections):\n        print(\n            f\"  - Mosquito {i+1}: \\\n            Confidence = {conf:.2f}, \\\n            Box = (x1={x1:.1f}, y1={y1:.1f}, x2={x2:.1f}, y2={y2:.1f})\",\n        )\nelse:\n    print(\"  No mosquitoes detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluating a Prediction with Ground Truth\n\nThe `evaluate` method allows you to compare a prediction against a ground truth.\nThis is useful for measuring the model's accuracy. The method returns several metrics,\nwhich are a standard for object detection.\nNow let's evaluate the prediction against the actual ground truth from the dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract ground truth boxes from the dataset sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ground_truth_boxes = []\nfor bbox in objects[\"bboxes\"]:\n    x_min, y_min, x_max, y_max = bbox\n    ground_truth_boxes.append((x_min, y_min, x_max, y_max))\n\n# Evaluate using the ground truth from the dataset\nprint(\"--- Evaluating with dataset ground truth ---\")\nevaluation = detector.evaluate(ground_truth=ground_truth_boxes, prediction=detections)\nprint(evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can let the method run prediction internally by passing the raw image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Evaluating directly from an image ---\")\nevaluation_from_raw = detector.evaluate(ground_truth=ground_truth_boxes, input_data=detect_image)\nprint(evaluation_from_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Running Batch Predictions on Dataset Images\n\nFor efficiency, you can process multiple images at once using `predict_batch`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract images from the detection dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image_batch = [sample[\"image\"] for sample in detect_data]\n\n# Run batch prediction\ndetections_batch = detector.predict_batch(image_batch)\nprint(\"Batch prediction complete.\")\n\nfor i, dets in enumerate(detections_batch):\n    print(f\"  - Image {i+1}: Found {len(dets)} detection(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluating a Batch of Predictions with Dataset Ground Truth\n\nSimilarly, `evaluate_batch` can be used to get aggregated metrics over the entire dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract ground truth from the detection dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ground_truth_batch = []\nfor sample in detect_data:\n    boxes = []\n    for bbox in sample[\"objects\"][\"bboxes\"]:\n        x_min, y_min, x_max, y_max = bbox\n        boxes.append((x_min, y_min, x_max, y_max))\n    ground_truth_batch.append(boxes)\n\n# Call evaluate_batch with dataset ground truth\nprint(\"\\n--- Evaluating the entire batch with dataset ground truth ---\")\nbatch_evaluation = detector.evaluate_batch(\n    ground_truth_batch=ground_truth_batch,\n    predictions_batch=detections_batch,\n    num_workers=2,  # Use multiple workers for faster processing\n)\n\nprint(\"Aggregated batch evaluation metrics:\")\nprint(batch_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizing Ground Truth vs Predictions\n\nLet's create a comparison visualization showing both ground truth and predictions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a function to visualize both ground truth and predictions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_comparison(image_rgb, ground_truth_boxes, detection_boxes):\n    # Draw ground truth boxes in green\n    for bbox in ground_truth_boxes:\n        x_min, y_min, x_max, y_max = (int(v) for v in bbox)\n        cv2.rectangle(image_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n        cv2.putText(\n            image_rgb,\n            \"GT\",\n            (x_min, y_min - 5),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (0, 255, 0),\n            1,\n        )\n\n    # Draw detection boxes in blue with confidence\n    for x1, y1, x2, y2, conf in detection_boxes:\n        x_min, y_min, x_max, y_max = int(x1), int(y1), int(x2), int(y2)\n        cv2.rectangle(image_rgb, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n        cv2.putText(\n            image_rgb,\n            f\"{conf:.2f}\",\n            (x_min, y_min - 5),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (255, 0, 0),\n            1,\n        )\n\n    return image_rgb\n\n\n# Create comparison visualization\ncomparison_image = visualize_comparison(np.array(detect_image), ground_truth_boxes, detections)\n\n# Display the comparison\nplt.figure(figsize=(12, 8))\nplt.imshow(comparison_image)\nplt.axis(\"off\")\nplt.title(\"Ground Truth vs Predictions\\nGreen: Ground Truth\\nRed: Predictions with Confidence\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}