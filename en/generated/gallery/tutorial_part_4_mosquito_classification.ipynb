{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classifying Mosquito Species\n\nThis tutorial demonstrates how to use the `MosquitoClassifier` from the CulicidaeLab\nlibrary to identify mosquito species from images. We will walk through the entire\nprocess, from loading the model to evaluating its performance on a batch of data.\n\nThis guide will cover:\n\n- **Initialization**: How to load the settings and the pre-trained model.\n- **Data Handling**: How to use the `DatasetsManager` to fetch sample data.\n- **Single Image Prediction**: How to classify a single mosquito image.\n- **Visualization**: How to interpret and visualize the model's predictions.\n- **Batch Evaluation**: How to measure the model's accuracy on a set of test images.\n- **Reporting**: How to generate and visualize a comprehensive performance report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install the `culicidaelab` library if not already installed\n!pip install -q culicidaelab\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialization and Setup\n\nOur first step is to set up the necessary components. We will initialize:\n\n- **`settings`**: An object that holds all library configuration, such as\n  model paths and confidence thresholds.\n- **`DatasetsManager`**: A helper class to download and manage the sample\n  datasets used in this tutorial.\n- **`MosquitoClassifier`**: The main class for our classification task. We'll\n  pass `load_model=True` to ensure the pre-trained model weights are downloaded\n  and loaded into memory immediately.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n# Import the required classes from the CulicidaeLab library\nfrom culicidaelab import (\n    DatasetsManager,\n    MosquitoClassifier,\n    get_settings,\n)\n\n# Get the default library settings instance\nsettings = get_settings()\n\n# Initialize the services needed to manage and download data\n\nmanager = DatasetsManager(settings)\n\n# Instantiate the classifier and load the model.\n# This might take a moment on the first run as it downloads the model weights.\nprint(\"Initializing MosquitoClassifier and loading model...\")\nclassifier = MosquitoClassifier(settings, load_model=True)\nprint(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspecting Model Classes\n\nBefore we start predicting, it's useful to know which species the model was\ntrained to recognize. We can easily access this information from the settings\nobject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "species_map = settings.species_config.species_map\nprint(f\"--- The model can recognize {len(species_map)} classes ---\")\n# Print the first 5 for brevity\nfor idx, name in list(species_map.items())[:5]:\n    print(f\"  Class Index {idx}: {name}\")\nprint(\"  ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading the Test Dataset\n\nFor this tutorial, we will use a built-in test dataset provided by the library.\nThe `DatasetsManager` makes it simple to download and load this data. The dataset\ncontains images and their corresponding correct labels, which we will use for\nprediction and later for evaluation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Loading the 'classification' dataset's 'test' split ---\")\nclassification_test_data = manager.load_dataset(\"classification\", split=\"test\")\nprint(\"Test dataset loaded successfully!\")\nprint(f\"Number of samples in the test dataset: {len(classification_test_data)}\")\n\n# Let's select one sample to work with.\n# The sample is a dictionary containing the image and its ground truth label.\nsample_index = 287\nsample = classification_test_data[sample_index]\nimage = sample[\"image\"]\nground_truth_label = sample[\"label\"]\n\nprint(f\"\\nSelected sample's ground truth label: '{ground_truth_label}'\")\n\n# Display the input image\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.title(f\"Input Image\\n(Ground Truth: {ground_truth_label})\")\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Classifying a Single Image\n\nNow we'll use the classifier to predict the species of the mosquito in our\nselected image. The `predict()` method takes an image (as a NumPy array, file\npath, or PIL Image) and returns a list of predictions, sorted from most to\nleast confident.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the classification on our sample image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictions = classifier.predict(image)\n\n# Print the top 5 predictions in a readable format\nprint(\"--- Top 5 Predictions ---\")\nfor species, probability in predictions[:5]:\n    print(f\"{species}: {probability:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizing and Interpreting the Results\n\nA raw list of predictions is useful, but visualizations make the results much\neasier to understand. We'll create two plots:\n\n1.  **A Bar Plot**: This shows the model's confidence for every possible\n    species. It's great for seeing not just the top prediction, but also which\n    other species the model considered.\n2.  **A Composite Image**: This uses the built-in `visualize()` method to create\n    a clean image that displays the top predictions alongside the input image.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a bar plot to visualize the probabilities for all species\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n\n# The predictions are already sorted, so we can plot them directly\nspecies_names = [p[0] for p in predictions]\nprobabilities = [p[1] for p in predictions]\n\n# We'll reverse the lists (`[::-1]`) so the highest probability is at the top\nbars = plt.barh(species_names[::-1], probabilities[::-1])\n\n# Highlight the bars that meet our confidence threshold\nconf_threshold = settings.get_config(\"predictors.classifier.confidence\")\nfor bar in bars:\n    if bar.get_width() >= conf_threshold:\n        bar.set_color(\"teal\")\n    else:\n        bar.set_color(\"lightgray\")\n\n# Add a reference line for the confidence threshold\nplt.axvline(\n    x=conf_threshold,\n    color=\"red\",\n    linestyle=\"--\",\n    label=f\"Confidence Threshold ({conf_threshold:.0%})\",\n)\nplt.xlabel(\"Assigned Probability\")\nplt.title(\"Species Classification Probabilities\")\nplt.legend()\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's use the built-in visualizer for a clean presentation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "annotated_image = classifier.visualize(image, predictions)\n\n# Display the final annotated image\nplt.figure(figsize=(10, 6))\nplt.imshow(annotated_image)\nplt.title(\"Classification Result\")\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluating Model Performance on a Batch\n\nWhile classifying a single image is useful, a more rigorous test involves\nevaluating the model's performance across an entire dataset. The\n`evaluate_batch()` method is designed for this. It processes a batch of\nimages and their corresponding ground truth labels, then computes aggregate\nmetrics.\n\nThe result is a `report` dictionary containing key metrics like mean\naccuracy and a **confusion matrix**, which shows exactly where the model is\nsucceeding or failing.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's evaluate the first 30 images from the test set for this example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_samples_to_evaluate = 30\nbatch_samples = classification_test_data.select(range(num_samples_to_evaluate))\nbatch_images = [sample[\"image\"] for sample in batch_samples]\nground_truths = [sample[\"label\"] for sample in batch_samples]\n\nprint(f\"\\n--- Evaluating a batch of {len(batch_images)} images ---\")\n\n# Run the batch evaluation.\n# The method can take images and ground truths separately, or it can\n# run predictions internally if you only provide the images.\nreport = classifier.evaluate_batch(\n    input_data_batch=batch_images,\n    ground_truth_batch=ground_truths,\n    show_progress=True,\n)\n\nprint(\"\\n--- Evaluation Report Summary ---\")\nfor key, value in report.items():\n    if key != \"confusion_matrix\":\n        # Check if value is a float before formatting\n        if isinstance(value, float):\n            print(f\"  {key}: {value:.4f}\")\n        else:\n            print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizing the Evaluation Report\n\nThe generated `report` dictionary contains a wealth of information, but the\nconfusion matrix is best understood visually. The `visualize_report()` method\ncreates a comprehensive plot that summarizes the evaluation results.\n\n**How to read the confusion matrix:**\n- Each row represents the *actual* ground truth species.\n- Each column represents the species that the *model predicted*.\n- The diagonal (from top-left to bottom-right) shows the number of correct\n  predictions for each class.\n- Off-diagonal numbers indicate misclassifications. For example, a number\n  in row \"A\" and column \"B\" means an image of species A was incorrectly\n  classified as species B.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pass the report dictionary to the visualization function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classifier.visualize_report(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Batch Prediction for Efficiency\n\nIf your goal is simply to classify many images (and not to evaluate\nperformance), using `predict_batch()` is much more efficient than looping\nover `predict()`. It leverages the GPU to process images in parallel,\nresulting in a significant speed-up.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the same small batch from our evaluation example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    f\"\\n--- Classifying a batch of {len(batch_images)} images with predict_batch ---\",\n)\nbatch_predictions = classifier.predict_batch(batch_images, show_progress=True)\n\nprint(\"\\n--- Batch Classification Results (Top prediction for each image) ---\")\nfor i, single_image_preds in enumerate(batch_predictions):\n    if single_image_preds:  # Check if the prediction list is not empty\n        top_pred_species = single_image_preds[0][0]\n        top_pred_conf = single_image_preds[0][1]\n        print(\n            f\"  - Image {i+1} (GT: {ground_truths[i]}): \"\n            f\"Predicted '{top_pred_species}' with {top_pred_conf:.2%} confidence.\",\n        )\n    else:\n        print(f\"  - Image {i+1} (GT: {ground_truths[i]}): Prediction failed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}