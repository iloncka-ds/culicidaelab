{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Mosquito Detection Tutorial\n\nThis tutorial shows how to use the `MosquitoDetector` from the CulicidaeLab\nlibrary to perform object detection on images. We will cover:\n\n- Loading the detector model\n- Preparing an image from the dataset\n- Running the model to get bounding boxes\n- Visualizing the results\n- Evaluating prediction accuracy\n- Running predictions on a batch of images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install the `culicidaelab` library if not already installed\n```bash\n!pip install -q culicidaelab[full]\n```\nor, if you have access to GPU\n```bash\n!pip install -q culicidaelab[full-gpu]\n```\n\n## 1. Initialization\n\nFirst, we'll get the global `settings` instance and use it to initialize our `MosquitoDetector`.\nBy setting `load_model=True`, we tell the detector to load the model weights into memory immediately.\nIf the model file doesn't exist locally, it will be downloaded automatically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom culicidaelab import get_settings\nfrom culicidaelab import MosquitoDetector, DatasetsManager\n\n# Get settings instance\nsettings = get_settings()\n\n# Initialize the datasets manager\nmanager = DatasetsManager(settings)\n\n# Load detection dataset\ndetect_data = manager.load_dataset(\"detection\", split=\"train[:20]\")\n\n# Instantiate the detector and load the model\nprint(\"Initializing MosquitoDetector and loading model...\")\ndetector = MosquitoDetector(settings=settings, load_model=True)\nprint(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Detecting Mosquitoes in a Dataset Image\n\nNow let's use an image from the detection dataset and run the detector on it.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect a detection sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "detect_sample = detect_data[5]\ndetect_image = detect_sample[\"image\"]\n\n# Get ground truth objects\nobjects = detect_sample[\"objects\"]\nprint(f\"Found {len(objects['bboxes'])} object(s) in this image.\")\n\n# The `predict` method returns a list of detections.\n# Each detection is a tuple: (x1, y1, x2, y2, confidence_score)\nresult = detector.predict(detect_image)\n\n# The `visualize` method draws the bounding boxes onto the image for easy inspection.\nannotated_image = detector.visualize(detect_image, result)\n\n# Display the result\nplt.figure(figsize=(12, 8))\nplt.imshow(annotated_image)\nplt.axis(\"off\")\nplt.title(\"Detected Mosquitoes\")\nplt.show()\n\n# Print the numerical detection results\nprint(\"\\nDetection Results:\")\nif result:\n    for i, det in enumerate(result.detections):\n        print(\n            f\"  - Mosquito {i+1}: \\\n            Confidence = {det.confidence:.2f}, \\\n            Box = (x1={det.box.x1:.1f}, y1={det.box.y1:.1f}, x2={det.box.x2:.1f}, y2={det.box.y2:.1f})\",\n        )\nelse:\n    print(\"  No mosquitoes detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluating a Prediction with Ground Truth\n\nThe `evaluate` method allows you to compare a prediction against a ground truth.\nThis is useful for measuring the model's accuracy. The method returns several metrics,\nwhich are a standard for object detection.\nNow let's evaluate the prediction against the actual ground truth from the dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract ground truth boxes from the dataset sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ground_truth_boxes = []\nfor bbox in objects[\"bboxes\"]:\n    x_min, y_min, x_max, y_max = bbox\n    ground_truth_boxes.append((x_min, y_min, x_max, y_max))\n\n# Evaluate using the ground truth from the dataset\nprint(\"--- Evaluating with dataset ground truth ---\")\nevaluation = detector.evaluate(ground_truth=ground_truth_boxes, prediction=result)\nprint(evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can let the method run prediction internally by passing the raw image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Evaluating directly from an image ---\")\nevaluation_from_raw = detector.evaluate(ground_truth=ground_truth_boxes, input_data=detect_image)\nprint(evaluation_from_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Running Batch Predictions on Dataset Images\n\nFor efficiency, you can process multiple images at once using `predict_batch`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract images from the detection dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image_batch = [sample[\"image\"] for sample in detect_data]\n\n# Run batch prediction\ndetections_batch = detector.predict_batch(image_batch)\nprint(\"Batch prediction complete.\")\n\nfor i, dets in enumerate(detections_batch):\n    print(f\"  - Image {i+1}: Found {len(dets.detections)} detection(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluating a Batch of Predictions with Dataset Ground Truth\n\nSimilarly, `evaluate_batch` can be used to get aggregated metrics over the entire dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract ground truth from the detection dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ground_truth_batch = []\nfor sample in detect_data:\n    boxes = []\n    for bbox in sample[\"objects\"][\"bboxes\"]:\n        x_min, y_min, x_max, y_max = bbox\n        boxes.append((x_min, y_min, x_max, y_max))\n    ground_truth_batch.append(boxes)\n\n# Call evaluate_batch with dataset ground truth\nprint(\"\\n--- Evaluating the entire batch with dataset ground truth ---\")\nbatch_evaluation = detector.evaluate_batch(\n    ground_truth_batch=ground_truth_batch,\n    predictions_batch=detections_batch,\n    num_workers=2,  # Use multiple workers for faster processing\n)\n\nprint(\"Aggregated batch evaluation metrics:\")\nprint(batch_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizing Ground Truth vs Predictions\n\nLet's create a comparison visualization showing both ground truth and predictions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a function to visualize both ground truth and predictions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_comparison(image_rgb, ground_truth_boxes, detections):\n    \"\"\"\n    Visualize ground truth and detection bounding boxes on an image using Pillow.\n\n    Args:\n        image_rgb: RGB image as numpy array or PIL Image\n        ground_truth_boxes: List of ground truth bounding boxes [x_min, y_min, x_max, y_max]\n        detections: List of detections results\n\n    Returns:\n        PIL Image with bounding boxes drawn\n    \"\"\"\n    # Convert numpy array to PIL Image if needed\n    if isinstance(image_rgb, np.ndarray):\n        if image_rgb.dtype == np.float32 or image_rgb.dtype == np.float64:\n            image_rgb = (image_rgb * 255).astype(np.uint8)\n        image = Image.fromarray(image_rgb)\n    else:\n        image = image_rgb.copy()\n\n    # Create a drawing context\n    draw = ImageDraw.Draw(image)\n\n    # Try to load a default font, fall back to default if not available\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 16)\n    except OSError:\n        try:\n            font = ImageFont.load_default()\n        except Exception:\n            font = None\n\n    # Draw ground truth boxes in green\n    for bbox in ground_truth_boxes:\n        x_min, y_min, x_max, y_max = (int(v) for v in bbox)\n\n        # Draw rectangle\n        draw.rectangle(\n            [(x_min, y_min), (x_max, y_max)],\n            outline=\"green\",\n            width=2,\n        )\n\n        # Draw label\n        text = \"GT\"\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n\n        # Position text above the box\n        text_x = x_min\n        text_y = max(0, y_min - text_height - 2)\n\n        # Draw text background\n        draw.rectangle(\n            [(text_x, text_y), (text_x + text_width, text_y + text_height)],\n            fill=\"green\",\n        )\n\n        # Draw text\n        draw.text((text_x, text_y), text, fill=\"white\", font=font)\n\n    # Draw detection boxes in blue with confidence\n    for det in detections.detections:\n        x_min, y_min, x_max, y_max = int(det.box.x1), int(det.box.y1), int(det.box.x2), int(det.box.y2)\n\n        # Draw rectangle\n        draw.rectangle(\n            [(x_min, y_min), (x_max, y_max)],\n            outline=\"red\",\n            width=2,\n        )\n\n        # Draw confidence label\n        text = f\"{det.confidence:.2f}\"\n        text_bbox = draw.textbbox((0, 0), text, font=font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n\n        # Position text above the box\n        text_x = x_min\n        text_y = max(0, y_min - text_height - 2)\n\n        # Draw text background\n        draw.rectangle(\n            [(text_x, text_y), (text_x + text_width, text_y + text_height)],\n            fill=\"red\",\n        )\n\n        # Draw text\n        draw.text((text_x, text_y), text, fill=\"white\", font=font)\n\n    return image\n\n\n# Create comparison visualization\ncomparison_image = visualize_comparison(np.array(detect_image), ground_truth_boxes, result)\n\n# Display the comparison\nplt.figure(figsize=(12, 8))\nplt.imshow(comparison_image)\nplt.axis(\"off\")\nplt.title(\"Ground Truth vs Predictions\\nGreen: Ground Truth\\nRed: Predictions with Confidence\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}