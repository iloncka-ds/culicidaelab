{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n#  Using the Serve Module for Production Inference\n\nThis tutorial demonstrates how to use the `serve` function from the CulicidaeLab\nlibrary for high-performance, production-ready inference. The `serve` function\nis designed to be a lightweight, fast, and safe way to run predictions.\n\nThis guide will cover:\n\n- **Speed and Safety**: How the `serve` function uses the ONNX backend for fast inference.\n- **Single Image Prediction**: How to use `serve` for classification.\n- **Caching**: Understand the in-memory caching for predictor instances.\n- **Clearing the Cache**: How to clear the cache when needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install the `culicidaelab` library if not already installed\n```bash\n!pip install -q culicidaelab[full]\n```\n## 1. Initialization and Setup\n\nWe will initialize the `DatasetsManager` to get some sample data.\nThe `serve` function doesn't require manual initialization of predictors.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n# Import the required classes from the CulicidaeLab library\nfrom culicidaelab import (\n    DatasetsManager,\n    get_settings,\n    serve,\n    clear_serve_cache,\n)\n\n# Get the default library settings instance\nsettings = get_settings()\n\n# Initialize the services needed to manage and download data\nmanager = DatasetsManager(settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading the Test Dataset\n\nWe will use a built-in test dataset to get an image for our predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"--- Loading the 'classification' dataset's 'test' split ---\")\nclassification_test_data = manager.load_dataset(\"classification\", split=\"test\")\nprint(\"Test dataset loaded successfully!\")\nprint(f\"Number of samples in the test dataset: {len(classification_test_data)}\")\n\n# Let's select one sample to work with.\nclassification_test_data = classification_test_data.shuffle(seed=42)\nsample = classification_test_data[0]\nimage = sample[\"image\"]\nground_truth_label = sample[\"label\"]\n\nprint(f\"\\nSelected sample's ground truth label: '{ground_truth_label}'\")\n\n# Display the input image\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.title(f\"Input Image\\n(Ground Truth: {ground_truth_label})\")\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using `serve` for Classification\n\nThe `serve` function automatically initializes the predictor with the ONNX backend\non the first call and caches it for subsequent requests.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run classification using the serve function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"--- Running classification for the first time (will initialize predictor) ---\")\nclassification_result = serve(image)\n\n# Print the top 5 predictions\nprint(\"\\n--- Top 5 Classification Predictions ---\")\nfor p in classification_result.predictions[:5]:\n    print(f\"{p.species_name}: {p.confidence:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Caching in Action\n\nIf you run the same request again, you'll notice it's much faster because\nthe predictor is already in memory.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run classification again to see the caching effect\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Running classification for the second time (should be faster) ---\")\nclassification_result_cached = serve(image, predictor_type=\"classifier\")\n\n# Print the top 5 predictions again\nprint(\"\\n--- Top 5 Classification Predictions (from cache) ---\")\nfor p in classification_result_cached.predictions[:5]:\n    print(f\"{p.species_name}: {p.confidence:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Clearing the Cache\n\nIf you need to free up memory or reload the predictors, you can use the\n`clear_serve_cache` function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clear the cache\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Clearing the predictor cache ---\")\nclear_serve_cache()\n\n# Run classification again, it will re-initialize the predictor\nprint(\"\\n--- Running classification again after clearing cache (will re-initialize) ---\")\nclassification_result_after_clear = serve(image)\n\nprint(\"\\n--- Top 5 Classification Predictions (after cache clear) ---\")\nfor p in classification_result_after_clear.predictions[:5]:\n    print(f\"{p.species_name}: {p.confidence:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}